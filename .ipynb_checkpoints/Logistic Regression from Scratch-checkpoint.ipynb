{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regressionfrom Scratch\n",
    "  \n",
    "  ### Main components of a Logistic Regression Code\n",
    "  1. __Data__: Make sure the input data is in the right format for the Algorithm\n",
    "  \n",
    "  \n",
    "  2. __Propagate__: Takes in the weights, X, and y andreturns the gradients and cost function\n",
    "  \n",
    "  \n",
    "  3. __Optimize__: This function takes in the X,y data, initialized weights, and number of iterations, and learening rate and returns the updated weights.\n",
    "  \n",
    "  \n",
    "  4. __Predict__: This function predicts the y given, X,w, and b\n",
    "  \n",
    "  \n",
    "  5. __Model__: This is like a sklearn model, takes in X_train,y_train, X_test, y_test and hyperparameters and returns the updated weights and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image_index, X, y):\n",
    "    if image_index > len(y):\n",
    "        raise IndexError('Index out of range: Index should be between 0 and {}'.format(len(y)))\n",
    "    print(y[image_index])\n",
    "    plt.imshow(X[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image(5000, x_train_raw, y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for 5 vs non-5 classifier\n",
    "y_train = [y==5 for y in y_train_raw]\n",
    "y_test = [y==5 for y in y_test_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training and testing examples\n",
    "m_train = x_train.shape[0]\n",
    "m_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "# create the X_train matrix\n",
    "x_train_flat = x_train.reshape(m_train,-1).T\n",
    "print(x_train_flat.shape)\n",
    "x_test_flat = x_test.reshape(m_test,-1).T\n",
    "print(x_test_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X training image values .... [0.         0.99215686 0.99607843 ... 0.         0.         0.57647059]\n",
      "X testing image values .... [0.99607843 0.         0.         ... 0.11764706 0.99607843 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# normalize the pixels\n",
    "x_train_set = x_train_flat/255\n",
    "x_test_set = x_test_flat/255\n",
    "\n",
    "print(\"X training image values ....\",x_train_set[300])\n",
    "print(\"X testing image values ....\",x_test_set[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - General Architecture of the learning algorithm ##\n",
    "\n",
    "It's time to design a simple algorithm to distinguish cat images from non-cat images.\n",
    "\n",
    "You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**\n",
    "\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Key steps**:\n",
    "In this exercise, you will carry out the following steps: \n",
    "    - Initialize the parameters of the model\n",
    "    - Learn the parameters for the model by minimizing the cost  \n",
    "    - Use the learned parameters to make predictions (on the test set)\n",
    "    - Analyse the results and conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights\n",
    "\n",
    "def initialize_weights(dim, init_type=0):\n",
    "    \"\"\"\n",
    "    This function simply takes in the dimension of the feature vector to intiaize weights\n",
    "    init_type: 0 means zero initializetion\n",
    "               1 means random initialization\n",
    "    \"\"\"\n",
    "    \n",
    "    if init_type == 0:\n",
    "        w = np.zeros((dim,1))\n",
    "        b = 0\n",
    "    else:\n",
    "        w = np.random.rand(dim,1)\n",
    "        b = random.random()\n",
    "        \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_gradients(w, b, X, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    num_samples = X.shape[0]\n",
    "    \n",
    "    A = sigmoid( np.dot(w.T,X) + b )\n",
    "    \n",
    "    ylogA = np.multiply(y,np.log(A))\n",
    "    _1_ylog1_A = (1-y)*np.log(1-A)\n",
    "    \n",
    "    LogLoss = ylogA + _1_ylog1_A\n",
    "    cost = (1/num_samples)*np.sum(LogLoss)\n",
    "    \n",
    "    dw = (1/m)*np.dot(X, (A-Y).T)\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "    \n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "w, b = initialize_weights(784, 0)\n",
    "A = propagate_gradients(w, b, x_test_set, y_test)\n",
    "print(A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
