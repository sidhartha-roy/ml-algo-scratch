{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regressionfrom Scratch\n",
    "  \n",
    "  ### Main components of a Logistic Regression Code\n",
    "  1. __Data__: Make sure the input data is in the right format for the Algorithm\n",
    "  \n",
    "  \n",
    "  2. __Propagate__: Takes in the weights, X, and y andreturns the gradients and cost function\n",
    "  \n",
    "  \n",
    "  3. __Optimize__: This function takes in the X,y data, initialized weights, and number of iterations, and learening rate and returns the updated weights.\n",
    "  \n",
    "  \n",
    "  4. __Predict__: This function predicts the y given, X,w, and b\n",
    "  \n",
    "  \n",
    "  5. __Model__: This is like a sklearn model, takes in X_train,y_train, X_test, y_test and hyperparameters and returns the updated weights and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = tf.keras.datasets.mnist.load_data()\n",
    "print(x_train_raw.shape)\n",
    "print(y_train_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image_index, X, y):\n",
    "    if image_index > len(y):\n",
    "        raise IndexError('Index out of range: Index should be between 0 and {}'.format(len(y)))\n",
    "    print(y[image_index])\n",
    "    plt.imshow(X[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image(5000, x_train_raw, y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape is (1, 60000)\n",
      "y_test shape is (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset for 5 vs non-5 classifier\n",
    "y_train = np.asarray([1 if y==5 else 0 for y in y_train_raw]).reshape(1,y_train_raw.shape[0])\n",
    "y_test = np.asarray([1 if y==5 else 0 for y in y_test_raw]).reshape(1,y_test_raw.shape[0])\n",
    "\n",
    "print(\"y_train shape is {}\".format(y_train.shape))\n",
    "print(\"y_test shape is {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training and testing examples\n",
    "m_train = x_train_raw.shape[0]\n",
    "m_test = x_test_raw.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "# create the X_train matrix\n",
    "x_train_flat = x_train_raw.reshape(m_train,-1).T\n",
    "print(x_train_flat.shape)\n",
    "x_test_flat = x_test_raw.reshape(m_test,-1).T\n",
    "print(x_test_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X training image values .... [0.         0.99215686 0.99607843 ... 0.         0.         0.57647059]\n",
      "X testing image values .... [0.99607843 0.         0.         ... 0.11764706 0.99607843 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# normalize the pixels\n",
    "x_train_set = x_train_flat/255\n",
    "x_test_set = x_test_flat/255\n",
    "\n",
    "print(\"X training image values ....\",x_train_set[300])\n",
    "print(\"X testing image values ....\",x_test_set[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - General Architecture of the learning algorithm ##\n",
    "\n",
    "It's time to design a simple algorithm to distinguish cat images from non-cat images.\n",
    "\n",
    "You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**\n",
    "\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Key steps**:\n",
    "In this exercise, you will carry out the following steps: \n",
    "    - Initialize the parameters of the model\n",
    "    - Learn the parameters for the model by minimizing the cost  \n",
    "    - Use the learned parameters to make predictions (on the test set)\n",
    "    - Analyse the results and conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights\n",
    "\n",
    "def initialize_weights(dim, init_type=0):\n",
    "    \"\"\"\n",
    "    This function simply takes in the dimension of the feature vector to intiaize weights\n",
    "    init_type: 0 means zero initializetion\n",
    "               1 means random initialization\n",
    "    \"\"\"\n",
    "    \n",
    "    if init_type == 0:\n",
    "        w = np.zeros((dim,1))\n",
    "        b = 0\n",
    "    else:\n",
    "        w = np.random.rand(dim,1)\n",
    "        b = random.random()\n",
    "        \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_gradients(w, b, X, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    num_samples = X.shape[1]\n",
    "    \n",
    "    A = sigmoid( np.dot(w.T,X) + b )\n",
    "    \n",
    "    # please refer to the formula of loss for logistic regression\n",
    "    ylogA = np.multiply(y,np.log(A))\n",
    "    _1_ylog1_A = np.multiply( 1-y, np.log(1-A) )\n",
    "    LogLoss = ylogA + _1_ylog1_A\n",
    "    \n",
    "    cost = (-1/num_samples)*np.sum(LogLoss)\n",
    "    \n",
    "    dw = (1/num_samples)*np.dot(X, (A-y).T)\n",
    "    db = (1/num_samples)*np.sum(A-y)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\n",
    "        'dw': dw,\n",
    "        'db': db\n",
    "    }\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate_gradients(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, y, num_iterations, lr, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function is used to optimize the w and b parameters by using the gradient descent algorithm\n",
    "    \n",
    "    Process: Gradient descent in Logistic Regression takes in w, b, X, y and uses propagate \n",
    "    gradients to update the w and b parameters.\n",
    "    \n",
    "    The only new thing that happens here is that the gradient propagation function is called multiple times\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate_gradients(w, b, X, y)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w -= np.multiply(lr, dw)\n",
    "        b -= lr*db\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "            if print_cost:\n",
    "                print(\"Cost after iteration {} is {:02.2f}\".format(i,cost))\n",
    "                \n",
    "                \n",
    "    params = {\n",
    "        'w':w,\n",
    "        'b':b\n",
    "    }\n",
    "        \n",
    "    grads = {\n",
    "        'dw':dw,\n",
    "        'db':db\n",
    "    }\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0 is 5.80\n",
      "w = [[0.19033591]\n",
      " [0.12259159]]\n",
      "b = 1.9253598300845747\n",
      "dw = [[0.67752042]\n",
      " [1.41625495]]\n",
      "db = 0.21919450454067652\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "\n",
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, lr = 0.009, print_cost = True)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \"\"\"\n",
    "    predict y given X\n",
    "    \"\"\"\n",
    "    #number of samples\n",
    "    m = X.shape[1]\n",
    "\n",
    "    y_predicted = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    \n",
    "    A = sigmoid( np.dot(w.T, X) + b )\n",
    "    \n",
    "    for i in range(m):\n",
    "        if A[0,i] > 0.5:\n",
    "            y_predicted[0,i] = 1\n",
    "        else:\n",
    "            y_predicted[0,i] = 0\n",
    "    \n",
    "    assert(y_predicted.shape == (1,m))\n",
    "    \n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m 3\n",
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_accuracy(predicted_labels, true_labels):\n",
    "    return np.mean(predicted_labels == true_labels)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test, num_iterations=2000, lr=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function primarily calls the optimize function to find the w and b parmaters\n",
    "    This also calculates the training and testing accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    N_features = X_train.shape[0]\n",
    "    m_train = y_train.shape[1]\n",
    "    m_test = y_test.shape[1]\n",
    "    \n",
    "    # 1. Initialize parameters\n",
    "    w, b = initialize_weights(N_features, init_type=1)\n",
    "    \n",
    "    # 2. Obtain optimized w* and b* parameters\n",
    "    params, grads, costs = optimize(w, b, X_train, y_train, num_iterations, lr, print_cost)\n",
    "    \n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    # 3. Use the w* and b* to make predictions on the training set\n",
    "    y_predict_train = predict(w, b, X_train)\n",
    "    y_predict_test = predict(w, b, X_test)\n",
    "\n",
    "    train_accuracy = classification_accuracy(y_predict_train,y_train)\n",
    "    test_accuracy = classification_accuracy(y_predict_test,y_test)\n",
    "    \n",
    "    print(\"Training Accuracy = {:2.3f}\".format(train_accuracy))\n",
    "    print(\"Testing Accuracy = {:2.3f}\".format(test_accuracy))\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": y_predict_test, \n",
    "         \"Y_prediction_train\" : y_predict_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : lr,\n",
    "         \"num_iterations\": num_iterations,\n",
    "         \"test_accuracy\":test_accuracy,\n",
    "         \"train_accuracy\":train_accuracy}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidroy/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n",
      "/home/sidroy/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0 is nan\n",
      "Cost after iteration 100 is 0.73\n",
      "Cost after iteration 200 is 0.43\n",
      "Cost after iteration 300 is 0.31\n",
      "Cost after iteration 400 is 0.26\n",
      "Cost after iteration 500 is 0.23\n",
      "Cost after iteration 600 is 0.21\n",
      "Cost after iteration 700 is 0.20\n",
      "Cost after iteration 800 is 0.19\n",
      "Cost after iteration 900 is 0.18\n",
      "Cost after iteration 1000 is 0.17\n",
      "Cost after iteration 1100 is 0.17\n",
      "Cost after iteration 1200 is 0.16\n",
      "Cost after iteration 1300 is 0.16\n",
      "Cost after iteration 1400 is 0.15\n",
      "Cost after iteration 1500 is 0.15\n",
      "Cost after iteration 1600 is 0.15\n",
      "Cost after iteration 1700 is 0.15\n",
      "Cost after iteration 1800 is 0.14\n",
      "Cost after iteration 1900 is 0.14\n",
      "Training Accuracy = 95.15\n",
      "Testing Accuracy = 95.61\n"
     ]
    }
   ],
   "source": [
    "d = model(x_train_set, y_train, x_test_set, y_test, num_iterations = 2000, lr = 0.05, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
